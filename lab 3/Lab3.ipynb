{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "g1GznJhObXPk",
        "Ru8vllrMbgvL",
        "z0MWgLingzhw",
        "lrLs_T2Qd0kc",
        "Gr9BxL8zeBfv"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andrea-1704/AML-labs/blob/main/lab%203/Lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf3z5SqWZ91b"
      },
      "source": [
        "# Torch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbtEmI1AiTkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13f97125-3db7-4799-dc06-3714518b039b"
      },
      "source": [
        "!pip3 install 'tqdm'\n",
        "!pip3 install \"colorama\"\n",
        "\n",
        "import torch\n",
        "#use GPU if available\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #'cpu' # 'cuda' or 'cpu'\n",
        "print(DEVICE)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (0.4.6)\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czjvnq3FjBmh"
      },
      "source": [
        "# Download Dataset GTEA61"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGOhXMNqjMed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67914d71-d539-4e04-9bd1-11c88597e523"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "import sys, os\n",
        "\n",
        "#1YKfdhB9Xxh4pmND1V3gcm3Gyjc8v8idq\n",
        "if not os.path.isfile('/content/drive/MyDrive'):\n",
        "  #!gdown --id 1Z5RWA8yKIy0PvxMlScV-aAz22ITtivfk # 3-5 min\n",
        "  !jar xvf  \"/content//drive/MyDrive/GTEA61.zip\" > /dev/null 2>&1\n",
        "\n",
        "if not os.path.isdir('/content/GTEA61'):\n",
        "  print(\"Dataset doesn't exist\")\n",
        "\n",
        "#Weights\n",
        "if not os.path.isfile(\"/content/best_model_state_dict_rgb_split2.pth\"):\n",
        "  !gdown --id 1B7Xh6hQ9Py8fmL-pjmLzlCent6dnuex5 > /dev/null 2>&1 # 3-5 min\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1B7Xh6hQ9Py8fmL-pjmLzlCent6dnuex5\n",
            "From (redirected): https://drive.google.com/uc?id=1B7Xh6hQ9Py8fmL-pjmLzlCent6dnuex5&confirm=t&uuid=464c3bce-e6df-41f8-8b62-45810001a649\n",
            "To: /content/best_model_state_dict_rgb_split2.pth\n",
            "100% 163M/163M [00:04<00:00, 34.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8xcWfReaSd_"
      },
      "source": [
        "!git clone \"https://github.com/plana93/Homework_AIML.git\" > /dev/null 2>&1\n",
        "#!rm -r \"/content/Homework_AIML\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl6fSd3MXofW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2586dd20-40ef-4576-d555-a1a30f688f98"
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.backends import cudnn\n",
        "import torchvision\n",
        "!p\n",
        "from colorama import init\n",
        "from colorama import Fore, Back, Style\n",
        "\n",
        "from torchvision.models import resnet34\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"/content/Homework_AIML/\")\n",
        "import Homework_AIML\n",
        "from Homework_AIML import *\n",
        "\n",
        "from gtea_dataset import GTEA61, GTEA61_flow, GTEA61_2Stream\n",
        "from spatial_transforms import (Compose, ToTensor, CenterCrop, Scale, Normalize, MultiScaleCornerCrop,\n",
        "                                RandomHorizontalFlip)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: p: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1GznJhObXPk"
      },
      "source": [
        "#**Learning without Temporal information** (avgpool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy4KrHClbAmC"
      },
      "source": [
        "#MAIN PARAMs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# homework_step\n",
        "\n",
        "E' una variabile che ci serve per indicare che tipo di rete andiamo ad usare."
      ],
      "metadata": {
        "id": "G3kjEfKGKyyk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-tz9mHPbCYW"
      },
      "source": [
        "#homework_step = 0 #--> Learning without Temporal information (avgpool)\n",
        "#homework_step = 1 #--> Learning with Temporal information (LSTM)\n",
        "homework_step = 2 #--> Learning with Spatio-Temporal information (ConvLSTM)\n",
        "\n",
        "\n",
        "\n",
        "DATA_DIR = '/content/GTEA61/' #path dataset\n",
        "model_folder = '/content/saved_models/' + \"/\" + \"homework_step\"+ str(homework_step) + \"/\" #path to save model\n",
        "if not os.path.isdir(model_folder):\n",
        "    os.makedirs(model_folder)\n",
        "\n",
        "\n",
        "# All this param can be change!\n",
        "\n",
        "NUM_CLASSES = 61\n",
        "BATCH_SIZE = 64\n",
        "LR = 0.001            # The initial Learning Rate\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 4e-5  # Regularization, you can keep this at the default\n",
        "NUM_EPOCHS = 50     # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = [25, 75, 150] # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "MEM_SIZE = 512       # Dim of internal state of LSTM or ConvLSTM\n",
        "SEQ_LEN = 3          # Num Frames\n",
        "\n",
        "# this dictionary is needed for the logger class\n",
        "parameters = {'DEVICE':DEVICE, 'NUM_CLASSES':NUM_CLASSES, 'BATCH_SIZE':BATCH_SIZE,\n",
        "             'LR':LR, 'MOMENTUM':MOMENTUM, 'WEIGHT_DECAY':WEIGHT_DECAY, 'NUM_EPOCHS':NUM_EPOCHS,\n",
        "             'STEP_SIZE':STEP_SIZE, 'GAMMA':GAMMA, 'MEM_SIZE':MEM_SIZE, 'SEQ_LEN':SEQ_LEN}"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPwkOR8taVdN"
      },
      "source": [
        "#Dataloaders & Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LT_Gy79SgBLq"
      },
      "source": [
        "# Normalize\n",
        "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "spatial_transform = Compose([Scale(256), RandomHorizontalFlip(), MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224),\n",
        "                             ToTensor(), normalize])\n",
        "spatial_transform_val = Compose([Scale(256), CenterCrop(224), ToTensor(), normalize])\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T69vfGGhjKa_",
        "outputId": "fa9b7b5f-d564-48dc-c119-c2b282ce6aec"
      },
      "source": [
        "# Prepare Pytorch train/test Datasets\n",
        "train_dataset = GTEA61(DATA_DIR, split='train', transform=spatial_transform, seq_len=SEQ_LEN)\n",
        "test_dataset = GTEA61(DATA_DIR, split='test', transform=spatial_transform_val, seq_len=SEQ_LEN)\n",
        "\n",
        "# Check dataset sizes\n",
        "print('Train Dataset: {}'.format(len(train_dataset)))\n",
        "print('Test Dataset: {}'.format(len(test_dataset)))\n",
        "\n",
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['S1', 'S4', 'S3', 'S2']\n",
            "['S1', 'S4', 'S3', 'S2']\n",
            "Train Dataset: 341\n",
            "Test Dataset: 116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21wjiwvW-OPQ"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ricorda che flatten lo faccio dopo le convoluzioni prima delle Fully connected.\n",
        "\n",
        "LSTM ha dei parametri che metti nella init."
      ],
      "metadata": {
        "id": "cL_p67CiFayO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ptzgMijcLlVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nota che se fai scelte strane devi metterlo nel report."
      ],
      "metadata": {
        "id": "Xnsh-GCYMxXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OU13Zr70M8V6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMWuE-4SHxoY"
      },
      "source": [
        "import torch\n",
        "import resnetMod\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM definition"
      ],
      "metadata": {
        "id": "Pc0Q-zD-NKFR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb_g2bmTNPg8"
      },
      "source": [
        "import torch\n",
        "import resnetMod\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "\n",
        "# LSTM\n",
        "class MyLSTMCell(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyLSTMCell, self).__init__()\n",
        "        self.input_size=input_size\n",
        "        self.hidden_size=hidden_size\n",
        "        super(MyLSTMCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Definisci layer lineari per ciascun gate\n",
        "        self.W_i = nn.Linear(input_size, hidden_size)  # input gate\n",
        "        self.U_i = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "\n",
        "        self.W_f = nn.Linear(input_size, hidden_size)  # forget gate\n",
        "        self.U_f = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "\n",
        "        self.W_c = nn.Linear(input_size, hidden_size)  # cell state update\n",
        "        self.U_c = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "\n",
        "        self.W_o = nn.Linear(input_size, hidden_size)  # output gate\n",
        "        self.U_o = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "\n",
        "        # Funzioni di attivazione\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x, state):\n",
        "        if state is None:\n",
        "            state = (Variable(torch.randn(x.size(0), x.size(1)).cuda()),\n",
        "                     Variable(torch.randn(x.size(0), x.size(1)).cuda()))\n",
        "\n",
        "        h_prev, c_prev = state\n",
        "\n",
        "        # Calcolo del gate di ingresso (input gate)\n",
        "        i = self.sigmoid(self.W_i(x) + self.U_i(h_prev))\n",
        "\n",
        "        # Calcolo del gate di dimenticanza (forget gate)\n",
        "        f = self.sigmoid(self.W_f(x) + self.U_f(h_prev))\n",
        "\n",
        "        # Calcolo dell'aggiornamento dello stato della cella (cell update)\n",
        "        g = self.tanh(self.W_c(x) + self.U_c(h_prev))\n",
        "\n",
        "        # Calcolo del gate di output\n",
        "        o = self.sigmoid(self.W_o(x) + self.U_o(h_prev))\n",
        "\n",
        "        # Aggiorna lo stato della cella\n",
        "        c_next = f * c_prev + i * g\n",
        "\n",
        "        # Aggiorna lo stato nascosto\n",
        "        h_next = o * self.tanh(c_next)\n",
        "\n",
        "        # Ritorna il nuovo stato nascosto e lo stato della cella\n",
        "        return h_next, c_next\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nota che tramite la funzione __nn.Linear(in_features, out_features, bias)__ stiamo definendo un livello di rete neurale di tipo Fully connected. Questo livello presenta in_features neuroni in ingresso e out_features neuroni in uscita e l'eventuale bias se specificato (bias è una variabile booleana).\n",
        "\n",
        "Per capire il perché di queste formule rivedi come è definita internamente la rete neurale LSTM:\n"
      ],
      "metadata": {
        "id": "Cm55IoDuPG7W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "wYF0iUaIP8X4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perché specifico il bias solo per alcuni livelli?\n",
        "Perché nella formula il prodotto Wfxt e Ufht-1 (per esempio) usano lo stesso bias (vedi il calcolo di ft).\n",
        "Pytorch ci fa scrivere il codice come se fossero due livelli differenti (perché lo sono) ma usano lo steso bias.\n",
        "\n",
        "Quindi quando scrivi:\n",
        "        self.W_i = nn.Linear(input_size, hidden_size)  # input gate\n",
        "        self.U_i = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "Qui usiamo lo stesso bias bf per il calcolo."
      ],
      "metadata": {
        "id": "4izRc-DDQIqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Come noti qui:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "def forward(self, x, state):\n",
        "        if state is None:\n",
        "            state = (Variable(torch.randn(x.size(0), x.size(1)).cuda()),\n",
        "                     Variable(torch.randn(x.size(0), x.size(1)).cuda()))\n",
        "\n",
        "        h_prev, c_prev = state\n",
        "\n",
        "        # Calcolo del gate di ingresso (input gate)\n",
        "        i = self.sigmoid(self.W_i(x) + self.U_i(h_prev))\n",
        "\n",
        "        # Calcolo del gate di dimenticanza (forget gate)\n",
        "        f = self.sigmoid(self.W_f(x) + self.U_f(h_prev))\n",
        "\n",
        "        # Calcolo dell'aggiornamento dello stato della cella (cell update)\n",
        "        g = self.tanh(self.W_c(x) + self.U_c(h_prev))\n",
        "\n",
        "        # Calcolo del gate di output\n",
        "        o = self.sigmoid(self.W_o(x) + self.U_o(h_prev))\n",
        "\n",
        "        # Aggiorna lo stato della cella\n",
        "        c_next = f * c_prev + i * g\n",
        "\n",
        "        # Aggiorna lo stato nascosto\n",
        "        h_next = o * self.tanh(c_next)\n",
        "\n",
        "        # Ritorna il nuovo stato nascosto e lo stato della cella\n",
        "        return h_next, c_next\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        " si tratta davvero solo di applicare la definizione delle formule sopra riportate."
      ],
      "metadata": {
        "id": "2TLyFQv1RBET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conv LSTM"
      ],
      "metadata": {
        "id": "W10qMcUbNkOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ConvLSTM\n",
        "class MyConvLSTMCell(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, kernel_size=3, stride=1, padding=1):\n",
        "        super(MyConvLSTMCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.conv_i_xx = nn.Conv2d(input_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.conv_i_hh = nn.Conv2d(hidden_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                                   bias=False)\n",
        "\n",
        "        self.conv_f_xx = nn.Conv2d(input_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.conv_f_hh = nn.Conv2d(hidden_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                                   bias=False)\n",
        "\n",
        "        self.conv_c_xx = nn.Conv2d(input_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.conv_c_hh = nn.Conv2d(hidden_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                                   bias=False)\n",
        "\n",
        "        self.conv_o_xx = nn.Conv2d(input_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.conv_o_hh = nn.Conv2d(hidden_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                                   bias=False)\n",
        "\n",
        "        torch.nn.init.xavier_normal_(self.conv_i_xx.weight)\n",
        "        torch.nn.init.constant_(self.conv_i_xx.bias, 0)\n",
        "        torch.nn.init.xavier_normal_(self.conv_i_hh.weight)\n",
        "\n",
        "        torch.nn.init.xavier_normal_(self.conv_f_xx.weight)\n",
        "        torch.nn.init.constant_(self.conv_f_xx.bias, 0)\n",
        "        torch.nn.init.xavier_normal_(self.conv_f_hh.weight)\n",
        "\n",
        "        torch.nn.init.xavier_normal_(self.conv_c_xx.weight)\n",
        "        torch.nn.init.constant_(self.conv_c_xx.bias, 0)\n",
        "        torch.nn.init.xavier_normal_(self.conv_c_hh.weight)\n",
        "\n",
        "        torch.nn.init.xavier_normal_(self.conv_o_xx.weight)\n",
        "        torch.nn.init.constant_(self.conv_o_xx.bias, 0)\n",
        "        torch.nn.init.xavier_normal_(self.conv_o_hh.weight)\n",
        "\n",
        "    def forward(self, x, state):\n",
        "        # Se non viene passato uno stato, inizializzalo a zero\n",
        "        if state is None:\n",
        "            h_prev = torch.zeros(x.size(0), self.hidden_size, x.size(2), x.size(3), device=x.device)\n",
        "            c_prev = torch.zeros(x.size(0), self.hidden_size, x.size(2), x.size(3), device=x.device)\n",
        "        else:\n",
        "            h_prev, c_prev = state\n",
        "\n",
        "        # Calcolo del gate di ingresso (input gate)\n",
        "        i = torch.sigmoid(self.conv_i_xx(x) + self.conv_i_hh(h_prev))\n",
        "\n",
        "        # Calcolo del gate di dimenticanza (forget gate)\n",
        "        f = torch.sigmoid(self.conv_f_xx(x) + self.conv_f_hh(h_prev))\n",
        "\n",
        "        # Calcolo dell'aggiornamento dello stato della cella (cell update)\n",
        "        g = torch.tanh(self.conv_c_xx(x) + self.conv_c_hh(h_prev))\n",
        "\n",
        "        # Calcolo del gate di output\n",
        "        o = torch.sigmoid(self.conv_o_xx(x) + self.conv_o_hh(h_prev))\n",
        "\n",
        "        # Aggiorna lo stato della cella\n",
        "        c_next = f * c_prev + i * g\n",
        "\n",
        "        # Aggiorna lo stato nascosto\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "\n",
        "        # Ritorna il nuovo stato nascosto e lo stato della cella\n",
        "        return h_next, c_next\n"
      ],
      "metadata": {
        "id": "VQpmlcPvNl2P"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il convolutional LSTM è esattamente come lstm ma con la differenza che invece di utilizzare nn.Linear usa dei livelli convoluzionali."
      ],
      "metadata": {
        "id": "Sg28XnkGgGdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final model"
      ],
      "metadata": {
        "id": "2MLGiWflNtBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Network\n",
        "class ourModel(nn.Module):\n",
        "    def __init__(self, num_classes=61, mem_size=512, homework_step = 0 , DEVICE=\"\"):\n",
        "        super(ourModel, self).__init__()\n",
        "        self.DEVICE = DEVICE\n",
        "        self.num_classes = num_classes\n",
        "        self.resNet = resnetMod.resnet34(True, True)\n",
        "        self.mem_size = mem_size\n",
        "        self.weight_softmax = self.resNet.fc.weight\n",
        "        self.homework_step = homework_step\n",
        "        if self.homework_step == 1:\n",
        "          self.lstm_cell = MyLSTMCell(512, mem_size)\n",
        "        elif self.homework_step == 2:\n",
        "          self.lstm_cell = MyConvLSTMCell(512, mem_size)\n",
        "\n",
        "        self.avgpool = nn.AvgPool2d(7)\n",
        "        self.dropout = nn.Dropout(0.7)\n",
        "        self.fc = nn.Linear(mem_size, self.num_classes)\n",
        "        self.classifier = nn.Sequential(self.dropout, self.fc)\n",
        "    def forward(self, inputVariable):\n",
        "        #Learning without Temporal information (mean)\n",
        "        if self.homework_step == 0:\n",
        "            video_level_features = torch.zeros((inputVariable.size(1), self.mem_size)).to(self.DEVICE)\n",
        "            for t in range(inputVariable.size(0)):\n",
        "                #spatial_frame_feat: (bs, 512, 7, 7)\n",
        "                _, spatial_frame_feat, _ = self.resNet(inputVariable[t])\n",
        "                #frames_feat: (bs, 512)\n",
        "                frame_feat = self.avgpool(spatial_frame_feat).view(spatial_frame_feat.size(0), -1)\n",
        "                video_level_features = video_level_features + frame_feat\n",
        "\n",
        "            video_level_features = video_level_features / inputVariable.size(0)\n",
        "            logits = self.classifier(video_level_features)\n",
        "            return logits, video_level_features\n",
        "\n",
        "        #Learning with Temporal information (LSTM)\n",
        "        elif self.homework_step == 1:\n",
        "            state = ( torch.zeros((inputVariable.size(1), self.mem_size)).to(self.DEVICE),\n",
        "                     torch.zeros((inputVariable.size(1), self.mem_size)).to(self.DEVICE) )\n",
        "            for t in range(inputVariable.size(0)):\n",
        "                #spatial_frame_feat: (bs, 512, 7, 7)\n",
        "                _, spatial_frame_feat, _ = self.resNet(inputVariable[t])\n",
        "                #frames_feat: (bs, 512)\n",
        "                frame_feat = self.avgpool(spatial_frame_feat).view(state[1].size(0), -1)\n",
        "                state = self.lstm_cell(frame_feat, state)\n",
        "\n",
        "            video_level_features = state[1]\n",
        "            logits = self.classifier(video_level_features)\n",
        "            return logits, video_level_features\n",
        "\n",
        "        #Learning with Temporal information (ConvLSTM)\n",
        "        elif self.homework_step == 2:\n",
        "            state = (torch.zeros((inputVariable.size(1), self.mem_size, 7, 7)).to(self.DEVICE),\n",
        "                     torch.zeros((inputVariable.size(1), self.mem_size, 7, 7)).to(self.DEVICE))\n",
        "            for t in range(inputVariable.size(0)):\n",
        "                #spatial_frame_feat: (bs, 512, 7, 7)\n",
        "                _, spatial_frame_feat, _ = self.resNet(inputVariable[t])\n",
        "                state = self.lstm_cell(spatial_frame_feat, state)\n",
        "            video_level_features = self.avgpool(state[1]).view(state[1].size(0), -1)\n",
        "            logits = self.classifier(video_level_features)\n",
        "            return logits, video_level_features"
      ],
      "metadata": {
        "id": "hr2zfKRHNutJ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qui definiamo il cuore del modello. Controllando una variabile chiamata __homework_step__ andiamo ad indicare se vogliamo applicare:\n",
        "\n",
        "\n",
        "\n",
        "1.   ResNet34 (per le immagini) + average pooling + fully connected\n",
        "2.   ResNet34 (per le immagini) + average pooling + LSTM + fully connected\n",
        "3.   ResNet34 (per le immagini) + ConvLSTM + fully connected\n",
        "\n",
        "Nota che ci serve un average pooling per passare da livelli convoluzionali a livelli fully connected.\n",
        "\n",
        "Il fully connected ci serve per fare la predizione. Inoltre nota che eseguiamo Dropout come metodo di regolarizzazione."
      ],
      "metadata": {
        "id": "Pep4-en8S0_4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru8vllrMbgvL"
      },
      "source": [
        "#Build Model - Loss - Opt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZe-ZbEL7z3x"
      },
      "source": [
        "#CUDA_LAUNCH_BLOCKING=1\n",
        "validate = True\n",
        "\n",
        "model = ourModel(num_classes=NUM_CLASSES, mem_size=MEM_SIZE, homework_step=homework_step, DEVICE=DEVICE) #model\n",
        "\n",
        "#Train only the lstm cell and classifier\n",
        "model.train(False)\n",
        "for params in model.parameters():\n",
        "    params.requires_grad = False\n",
        "\n",
        "if homework_step > 0:\n",
        "    for params in model.lstm_cell.parameters():\n",
        "        params.requires_grad = True\n",
        "    model.lstm_cell.train(True)\n",
        "\n",
        "for params in model.classifier.parameters():\n",
        "    params.requires_grad = True\n",
        "model.classifier.train(True)\n",
        "\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "#model.load_state_dict(torch.load(\"/content/best_model_state_dict_rgb_split2.pth\", map_location=torch.device('cpu')), strict=True)\n",
        "\n",
        "\n",
        "#Loss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "#Opt\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer_fn = optim.Adam(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-4)\n",
        "#Scheduler\n",
        "optim_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qui congelo la backbone.\n",
        "\n",
        "Nota che infatti con:\n",
        "\n",
        "\n",
        "```\n",
        "model.train(False)\n",
        "for params in model.parameters():\n",
        "    params.requires_grad = False\n",
        "\n",
        "if homework_step > 0:\n",
        "    for params in model.lstm_cell.parameters():\n",
        "        params.requires_grad = True\n",
        "    model.lstm_cell.train(True)\n",
        "\n",
        "for params in model.classifier.parameters():\n",
        "    params.requires_grad = True\n",
        "model.classifier.train(True)\n",
        "```\n",
        "Stiamo dicendo che vogliamo congelare la backbone, ovvero la resnet, e addestrare solo lstm e il livello FC.\n"
      ],
      "metadata": {
        "id": "3fSKX15xdRfr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0MWgLingzhw"
      },
      "source": [
        "#Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-uE2A9eHmtn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a74fd38e-be9b-42e3-ce32-85b517fa6fc2"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "train_iter = 0\n",
        "val_iter = 0\n",
        "min_accuracy = 0\n",
        "\n",
        "trainSamples = len(train_dataset) - (len(train_dataset) % BATCH_SIZE)\n",
        "val_samples = len(test_dataset)\n",
        "iterPerEpoch = len(train_loader)\n",
        "val_steps = len(val_loader)\n",
        "cudnn.benchmark\n",
        "model_checkpoint = \"model\" #name\n",
        "\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_loss = 0\n",
        "    numCorrTrain = 0\n",
        "\n",
        "    #blocks to train\n",
        "    if homework_step > 0:\n",
        "        model.lstm_cell.train(True)\n",
        "    model.classifier.train(True)\n",
        "\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        train_iter += 1\n",
        "        optimizer_fn.zero_grad()\n",
        "\n",
        "        # (BS, Frames, C, W, H) --> (Frames, BS, C, W, H)\n",
        "        inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
        "        labelVariable = targets.to(DEVICE)\n",
        "\n",
        "        # feeds in model\n",
        "        output_label, _ = model(inputVariable)\n",
        "\n",
        "        # compute loss\n",
        "        loss = loss_fn(output_label, labelVariable)\n",
        "\n",
        "        # backward loss and optimizer step\n",
        "        loss.backward()\n",
        "        optimizer_fn.step()\n",
        "\n",
        "        #compute the training accuracy\n",
        "        _, predicted = torch.max(output_label.data, 1)\n",
        "        numCorrTrain += torch.sum(predicted == labelVariable.data).data.item()\n",
        "        step_loss = loss.data.item()\n",
        "        epoch_loss += step_loss\n",
        "\n",
        "    avg_loss = epoch_loss/iterPerEpoch\n",
        "    trainAccuracy = (numCorrTrain / trainSamples) * 100\n",
        "    #train_logger.add_epoch_data(epoch+1, trainAccuracy, avg_loss)\n",
        "    print(Fore.BLACK + 'Train: Epoch = {} | Loss = {:.3f} | Accuracy = {:.3f}'.format(epoch+1, avg_loss, trainAccuracy))\n",
        "    if validate:\n",
        "        if (epoch+1) % 1 == 0:\n",
        "            model.train(False)\n",
        "            val_loss_epoch = 0\n",
        "            numCorr = 0\n",
        "            for j, (inputs, targets) in enumerate(val_loader):\n",
        "                val_iter += 1\n",
        "                inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
        "                labelVariable = targets.to(DEVICE)\n",
        "\n",
        "                output_label, _ = model(inputVariable)\n",
        "                val_loss = loss_fn(output_label, labelVariable)\n",
        "                val_loss_step = val_loss.data.item()\n",
        "                val_loss_epoch += val_loss_step\n",
        "                _, predicted = torch.max(output_label.data, 1)\n",
        "                numCorr += torch.sum(predicted == labelVariable.data).data.item()\n",
        "                #val_logger.add_step_data(val_iter, numCorr, val_loss_step)\n",
        "\n",
        "            val_accuracy = (numCorr / val_samples) * 100\n",
        "            avg_val_loss = val_loss_epoch / val_steps\n",
        "\n",
        "            print(Fore.GREEN + 'Val: Epoch = {} | Loss {:.3f} | Accuracy = {:.3f}'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
        "            if val_accuracy > min_accuracy:\n",
        "                print(\"[||| NEW BEST on val||||]\")\n",
        "                save_path_model = os.path.join(model_folder, model_checkpoint)\n",
        "                torch.save(model.state_dict(), save_path_model)\n",
        "                min_accuracy = val_accuracy\n",
        "\n",
        "    optim_scheduler.step()\n",
        "\n",
        "print(Fore.CYAN + \"Best Acc --> \", min_accuracy)\n",
        "print(Fore.CYAN + \"Last Acc --> \", val_accuracy)\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[30mTrain: Epoch = 1 | Loss = 4.616 | Accuracy = 1.875\n",
            "\u001b[32mVal: Epoch = 1 | Loss 3.960 | Accuracy = 3.448\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 2 | Loss = 4.291 | Accuracy = 5.312\n",
            "\u001b[32mVal: Epoch = 2 | Loss 3.890 | Accuracy = 6.034\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 3 | Loss = 4.125 | Accuracy = 5.625\n",
            "\u001b[32mVal: Epoch = 3 | Loss 3.874 | Accuracy = 7.759\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 4 | Loss = 3.983 | Accuracy = 6.875\n",
            "\u001b[32mVal: Epoch = 4 | Loss 3.866 | Accuracy = 8.621\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 5 | Loss = 3.977 | Accuracy = 6.875\n",
            "\u001b[32mVal: Epoch = 5 | Loss 3.862 | Accuracy = 8.621\n",
            "\u001b[30mTrain: Epoch = 6 | Loss = 3.965 | Accuracy = 7.187\n",
            "\u001b[32mVal: Epoch = 6 | Loss 3.817 | Accuracy = 6.897\n",
            "\u001b[30mTrain: Epoch = 7 | Loss = 3.859 | Accuracy = 5.938\n",
            "\u001b[32mVal: Epoch = 7 | Loss 3.801 | Accuracy = 8.621\n",
            "\u001b[30mTrain: Epoch = 8 | Loss = 3.938 | Accuracy = 7.187\n",
            "\u001b[32mVal: Epoch = 8 | Loss 3.774 | Accuracy = 7.759\n",
            "\u001b[30mTrain: Epoch = 9 | Loss = 3.866 | Accuracy = 7.812\n",
            "\u001b[32mVal: Epoch = 9 | Loss 3.763 | Accuracy = 7.759\n",
            "\u001b[30mTrain: Epoch = 10 | Loss = 3.826 | Accuracy = 7.500\n",
            "\u001b[32mVal: Epoch = 10 | Loss 3.735 | Accuracy = 6.897\n",
            "\u001b[30mTrain: Epoch = 11 | Loss = 3.752 | Accuracy = 8.438\n",
            "\u001b[32mVal: Epoch = 11 | Loss 3.706 | Accuracy = 11.207\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 12 | Loss = 3.767 | Accuracy = 10.000\n",
            "\u001b[32mVal: Epoch = 12 | Loss 3.665 | Accuracy = 8.621\n",
            "\u001b[30mTrain: Epoch = 13 | Loss = 3.677 | Accuracy = 13.125\n",
            "\u001b[32mVal: Epoch = 13 | Loss 3.630 | Accuracy = 8.621\n",
            "\u001b[30mTrain: Epoch = 14 | Loss = 3.689 | Accuracy = 10.625\n",
            "\u001b[32mVal: Epoch = 14 | Loss 3.684 | Accuracy = 5.172\n",
            "\u001b[30mTrain: Epoch = 15 | Loss = 3.695 | Accuracy = 10.625\n",
            "\u001b[32mVal: Epoch = 15 | Loss 3.572 | Accuracy = 12.069\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 16 | Loss = 3.558 | Accuracy = 14.062\n",
            "\u001b[32mVal: Epoch = 16 | Loss 3.581 | Accuracy = 11.207\n",
            "\u001b[30mTrain: Epoch = 17 | Loss = 3.579 | Accuracy = 13.750\n",
            "\u001b[32mVal: Epoch = 17 | Loss 3.548 | Accuracy = 12.069\n",
            "\u001b[30mTrain: Epoch = 18 | Loss = 3.550 | Accuracy = 13.750\n",
            "\u001b[32mVal: Epoch = 18 | Loss 3.484 | Accuracy = 12.069\n",
            "\u001b[30mTrain: Epoch = 19 | Loss = 3.542 | Accuracy = 8.438\n",
            "\u001b[32mVal: Epoch = 19 | Loss 3.512 | Accuracy = 13.793\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 20 | Loss = 3.461 | Accuracy = 13.750\n",
            "\u001b[32mVal: Epoch = 20 | Loss 3.446 | Accuracy = 14.655\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 21 | Loss = 3.413 | Accuracy = 14.688\n",
            "\u001b[32mVal: Epoch = 21 | Loss 3.471 | Accuracy = 12.931\n",
            "\u001b[30mTrain: Epoch = 22 | Loss = 3.406 | Accuracy = 15.313\n",
            "\u001b[32mVal: Epoch = 22 | Loss 3.359 | Accuracy = 12.931\n",
            "\u001b[30mTrain: Epoch = 23 | Loss = 3.378 | Accuracy = 15.000\n",
            "\u001b[32mVal: Epoch = 23 | Loss 3.406 | Accuracy = 15.517\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 24 | Loss = 3.304 | Accuracy = 18.125\n",
            "\u001b[32mVal: Epoch = 24 | Loss 3.272 | Accuracy = 16.379\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 25 | Loss = 3.345 | Accuracy = 12.812\n",
            "\u001b[32mVal: Epoch = 25 | Loss 3.267 | Accuracy = 18.103\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 26 | Loss = 3.238 | Accuracy = 16.875\n",
            "\u001b[32mVal: Epoch = 26 | Loss 3.258 | Accuracy = 17.241\n",
            "\u001b[30mTrain: Epoch = 27 | Loss = 3.277 | Accuracy = 17.188\n",
            "\u001b[32mVal: Epoch = 27 | Loss 3.229 | Accuracy = 18.103\n",
            "\u001b[30mTrain: Epoch = 28 | Loss = 3.152 | Accuracy = 20.312\n",
            "\u001b[32mVal: Epoch = 28 | Loss 3.201 | Accuracy = 18.103\n",
            "\u001b[30mTrain: Epoch = 29 | Loss = 3.124 | Accuracy = 17.500\n",
            "\u001b[32mVal: Epoch = 29 | Loss 3.188 | Accuracy = 18.103\n",
            "\u001b[30mTrain: Epoch = 30 | Loss = 3.143 | Accuracy = 20.625\n",
            "\u001b[32mVal: Epoch = 30 | Loss 3.189 | Accuracy = 18.103\n",
            "\u001b[30mTrain: Epoch = 31 | Loss = 3.174 | Accuracy = 17.188\n",
            "\u001b[32mVal: Epoch = 31 | Loss 3.168 | Accuracy = 18.966\n",
            "[||| NEW BEST on val||||]\n",
            "\u001b[30mTrain: Epoch = 32 | Loss = 3.112 | Accuracy = 19.688\n",
            "\u001b[32mVal: Epoch = 32 | Loss 3.161 | Accuracy = 18.103\n",
            "\u001b[30mTrain: Epoch = 33 | Loss = 3.098 | Accuracy = 22.812\n",
            "\u001b[32mVal: Epoch = 33 | Loss 3.163 | Accuracy = 18.103\n",
            "\u001b[30mTrain: Epoch = 34 | Loss = 3.177 | Accuracy = 16.875\n",
            "\u001b[32mVal: Epoch = 34 | Loss 3.157 | Accuracy = 18.103\n",
            "\u001b[30mTrain: Epoch = 35 | Loss = 3.088 | Accuracy = 20.938\n",
            "\u001b[32mVal: Epoch = 35 | Loss 3.144 | Accuracy = 18.103\n",
            "\u001b[30mTrain: Epoch = 36 | Loss = 3.139 | Accuracy = 18.438\n",
            "\u001b[32mVal: Epoch = 36 | Loss 3.135 | Accuracy = 18.966\n",
            "\u001b[30mTrain: Epoch = 37 | Loss = 3.143 | Accuracy = 23.125\n",
            "\u001b[32mVal: Epoch = 37 | Loss 3.136 | Accuracy = 18.103\n",
            "\u001b[30mTrain: Epoch = 38 | Loss = 3.062 | Accuracy = 20.000\n",
            "\u001b[32mVal: Epoch = 38 | Loss 3.139 | Accuracy = 18.103\n",
            "\u001b[30mTrain: Epoch = 39 | Loss = 3.077 | Accuracy = 19.688\n",
            "\u001b[32mVal: Epoch = 39 | Loss 3.136 | Accuracy = 17.241\n",
            "\u001b[30mTrain: Epoch = 40 | Loss = 3.072 | Accuracy = 20.625\n",
            "\u001b[32mVal: Epoch = 40 | Loss 3.123 | Accuracy = 17.241\n",
            "\u001b[30mTrain: Epoch = 41 | Loss = 3.014 | Accuracy = 22.188\n",
            "\u001b[32mVal: Epoch = 41 | Loss 3.100 | Accuracy = 18.103\n",
            "\u001b[30mTrain: Epoch = 42 | Loss = 3.013 | Accuracy = 21.875\n",
            "\u001b[32mVal: Epoch = 42 | Loss 3.098 | Accuracy = 18.103\n",
            "\u001b[30mTrain: Epoch = 43 | Loss = 3.100 | Accuracy = 20.312\n",
            "\u001b[32mVal: Epoch = 43 | Loss 3.107 | Accuracy = 18.103\n",
            "\u001b[30mTrain: Epoch = 44 | Loss = 3.011 | Accuracy = 21.562\n",
            "\u001b[32mVal: Epoch = 44 | Loss 3.117 | Accuracy = 18.103\n",
            "\u001b[30mTrain: Epoch = 45 | Loss = 3.045 | Accuracy = 22.188\n",
            "\u001b[32mVal: Epoch = 45 | Loss 3.107 | Accuracy = 18.103\n",
            "\u001b[30mTrain: Epoch = 46 | Loss = 3.062 | Accuracy = 21.250\n",
            "\u001b[32mVal: Epoch = 46 | Loss 3.081 | Accuracy = 18.103\n",
            "\u001b[30mTrain: Epoch = 47 | Loss = 2.981 | Accuracy = 22.500\n",
            "\u001b[32mVal: Epoch = 47 | Loss 3.070 | Accuracy = 18.966\n",
            "\u001b[30mTrain: Epoch = 48 | Loss = 2.967 | Accuracy = 23.438\n",
            "\u001b[32mVal: Epoch = 48 | Loss 3.074 | Accuracy = 18.966\n",
            "\u001b[30mTrain: Epoch = 49 | Loss = 2.945 | Accuracy = 24.062\n",
            "\u001b[32mVal: Epoch = 49 | Loss 3.083 | Accuracy = 18.966\n",
            "\u001b[30mTrain: Epoch = 50 | Loss = 2.971 | Accuracy = 24.375\n",
            "\u001b[32mVal: Epoch = 50 | Loss 3.087 | Accuracy = 18.103\n",
            "\u001b[36mBest Acc -->  18.96551724137931\n",
            "\u001b[36mLast Acc -->  18.103448275862068\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrLs_T2Qd0kc"
      },
      "source": [
        "#Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqK1ExB0cl8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2acfd46-d5e1-45d9-96ec-a8eaf185b8ef"
      },
      "source": [
        "model.train(False)\n",
        "val_loss_epoch = 0\n",
        "numCorr = 0\n",
        "val_iter = 0\n",
        "val_samples = len(test_dataset)\n",
        "val_steps = len(val_loader)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for j, (inputs, targets) in enumerate(val_loader):\n",
        "        val_iter += 1\n",
        "        inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
        "        labelVariable = targets.to(DEVICE)\n",
        "\n",
        "        output_label, _ = model(inputVariable)\n",
        "        val_loss = loss_fn(output_label, labelVariable)\n",
        "        val_loss_step = val_loss.data.item()\n",
        "        val_loss_epoch += val_loss_step\n",
        "        _, predicted = torch.max(output_label.data, 1)\n",
        "        numCorr += torch.sum(predicted == labelVariable.data).data.item()\n",
        "\n",
        "    val_accuracy = (numCorr / val_samples) * 100\n",
        "    avg_val_loss = val_loss_epoch / val_steps\n",
        "\n",
        "print('Loss {:.3f} | Accuracy = {:.3f}'.format(avg_val_loss, val_accuracy))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss 3.087 | Accuracy = 18.103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-qHYgnyf_wn"
      },
      "source": [
        "# Load the pretrained-weights\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# definizione del modello:\n",
        "model = ourModel(num_classes=NUM_CLASSES, mem_size=MEM_SIZE, homework_step=homework_step, DEVICE=DEVICE) #model\n",
        "\n",
        "# definizione del percorso da cui vogliamo prendere i nostri parametri:\n",
        "weights_path = \"/content/best_model_state_dict_rgb_split2.pth\"\n",
        "\n",
        "#caricamento dei pesi dentro model:\n",
        "model.load_state_dict(torch.load(weights_path))\n"
      ],
      "metadata": {
        "id": "UpPCNE8NhkLJ",
        "outputId": "12c274c6-3026-4dd3-f02b-655b817330de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train only the convLSTM and the classifier layer:\n",
        "model.train(False)\n",
        "for params in model.parameters():\n",
        "    params.requires_grad = False\n",
        "\n",
        "for params in model.lstm_cell.parameters():\n",
        "      params.requires_grad = True\n",
        "model.lstm_cell.train(True)\n",
        "\n",
        "for params in model.classifier.parameters():\n",
        "    params.requires_grad = True\n",
        "model.classifier.train(True)\n"
      ],
      "metadata": {
        "id": "uORb1qHBjTL8",
        "outputId": "f5bd9cb4-a617-494f-9160-aadc2c8a6af3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Dropout(p=0.7, inplace=False)\n",
              "  (1): Linear(in_features=512, out_features=61, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training:\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "train_iter = 0\n",
        "val_iter = 0\n",
        "min_accuracy = 0\n",
        "\n",
        "trainSamples = len(train_dataset) - (len(train_dataset) % BATCH_SIZE)\n",
        "val_samples = len(test_dataset)\n",
        "iterPerEpoch = len(train_loader)\n",
        "val_steps = len(val_loader)\n",
        "cudnn.benchmark\n",
        "model_checkpoint = \"model\" #name\n",
        "\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_loss = 0\n",
        "    numCorrTrain = 0\n",
        "\n",
        "    #blocks to train\n",
        "    if homework_step > 0:\n",
        "        model.lstm_cell.train(True)\n",
        "    model.classifier.train(True)\n",
        "\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        train_iter += 1\n",
        "        optimizer_fn.zero_grad()\n",
        "\n",
        "        # (BS, Frames, C, W, H) --> (Frames, BS, C, W, H)\n",
        "        inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
        "        labelVariable = targets.to(DEVICE)\n",
        "\n",
        "        # feeds in model\n",
        "        output_label, _ = model(inputVariable)\n",
        "\n",
        "        # compute loss\n",
        "        loss = loss_fn(output_label, labelVariable)\n",
        "\n",
        "        # backward loss and optimizer step\n",
        "        loss.backward()\n",
        "        optimizer_fn.step()\n",
        "\n",
        "        #compute the training accuracy\n",
        "        _, predicted = torch.max(output_label.data, 1)\n",
        "        numCorrTrain += torch.sum(predicted == labelVariable.data).data.item()\n",
        "        step_loss = loss.data.item()\n",
        "        epoch_loss += step_loss\n",
        "\n",
        "    avg_loss = epoch_loss/iterPerEpoch\n",
        "    trainAccuracy = (numCorrTrain / trainSamples) * 100\n",
        "    #train_logger.add_epoch_data(epoch+1, trainAccuracy, avg_loss)\n",
        "    print(Fore.BLACK + 'Train: Epoch = {} | Loss = {:.3f} | Accuracy = {:.3f}'.format(epoch+1, avg_loss, trainAccuracy))\n",
        "    if validate:\n",
        "        if (epoch+1) % 1 == 0:\n",
        "            model.train(False)\n",
        "            val_loss_epoch = 0\n",
        "            numCorr = 0\n",
        "            for j, (inputs, targets) in enumerate(val_loader):\n",
        "                val_iter += 1\n",
        "                inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
        "                labelVariable = targets.to(DEVICE)\n",
        "\n",
        "                output_label, _ = model(inputVariable)\n",
        "                val_loss = loss_fn(output_label, labelVariable)\n",
        "                val_loss_step = val_loss.data.item()\n",
        "                val_loss_epoch += val_loss_step\n",
        "                _, predicted = torch.max(output_label.data, 1)\n",
        "                numCorr += torch.sum(predicted == labelVariable.data).data.item()\n",
        "                #val_logger.add_step_data(val_iter, numCorr, val_loss_step)\n",
        "\n",
        "            val_accuracy = (numCorr / val_samples) * 100\n",
        "            avg_val_loss = val_loss_epoch / val_steps\n",
        "\n",
        "            print(Fore.GREEN + 'Val: Epoch = {} | Loss {:.3f} | Accuracy = {:.3f}'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
        "            if val_accuracy > min_accuracy:\n",
        "                print(\"[||| NEW BEST on val||||]\")\n",
        "                save_path_model = os.path.join(model_folder, model_checkpoint)\n",
        "                torch.save(model.state_dict(), save_path_model)\n",
        "                min_accuracy = val_accuracy\n",
        "\n",
        "    optim_scheduler.step()\n",
        "\n",
        "print(Fore.CYAN + \"Best Acc --> \", min_accuracy)\n",
        "print(Fore.CYAN + \"Last Acc --> \", val_accuracy)\n"
      ],
      "metadata": {
        "id": "NLxtHjwCjd9E",
        "outputId": "3b3119d6-3d82-42ad-fb87-2df24c7ee3d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-facddc656752>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# feeds in model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0moutput_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-127388089a92>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputVariable)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;31m#spatial_frame_feat: (bs, 512, 7, 7)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspatial_frame_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputVariable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspatial_frame_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mvideo_level_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Homework_AIML/resnetMod.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
          ]
        }
      ]
    }
  ]
}